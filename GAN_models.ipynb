{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gc\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Mayank\\anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import applications\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import optimizers\n",
    "import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BaseMetrics(y_pred,y_true):\n",
    "    TP = np.sum( (y_pred == 1) & (y_true == 1) )\n",
    "    TN = np.sum( (y_pred == 0) & (y_true == 0) )\n",
    "    FP = np.sum( (y_pred == 1) & (y_true == 0) )\n",
    "    FN = np.sum( (y_pred == 0) & (y_true == 1) )\n",
    "    return TP, TN, FP, FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SimpleMetrics(y_pred,y_true):\n",
    "    TP, TN, FP, FN = BaseMetrics(y_pred,y_true)\n",
    "    ACC = ( TP + TN ) / ( TP + TN + FP + FN )\n",
    "    \n",
    "    # Reporting\n",
    "    from IPython.display import display\n",
    "    print( 'Confusion Matrix')\n",
    "    display( pd.DataFrame( [[TN,FP],[FN,TP]], columns=['Pred 0','Pred 1'], index=['True 0', 'True 1'] ) )\n",
    "    print( 'Accuracy : {}'.format( ACC ))\n",
    "    \n",
    "def SimpleAccuracy(y_pred,y_true):\n",
    "    TP, TN, FP, FN = BaseMetrics(y_pred,y_true)\n",
    "    ACC = ( TP + TN ) / ( TP + TN + FP + FN )\n",
    "    return ACC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def get_data_batch(train, batch_size, seed=0):\n",
    "    # # random sampling - some samples will have excessively low or high sampling, but easy to implement\n",
    "    # np.random.seed(seed)\n",
    "    # x = train.loc[ np.random.choice(train.index, batch_size) ].values\n",
    "    \n",
    "    # iterate through shuffled indices, so every sample gets covered evenly\n",
    "    start_i = (batch_size * seed) % len(train)\n",
    "    stop_i = start_i + batch_size\n",
    "    shuffle_seed = (batch_size * seed) // len(train)\n",
    "    np.random.seed(shuffle_seed)\n",
    "    train_ix = np.random.choice( list(train.index), replace=False, size=len(train) ) # wasteful to shuffle every time\n",
    "    train_ix = list(train_ix) + list(train_ix) # duplicate to cover ranges past the end of the set\n",
    "    x = train.loc[ train_ix[ start_i: stop_i ] ].values\n",
    "    \n",
    "    return np.reshape(x, (batch_size, -1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def CheckAccuracy( x, g_z, data_cols, label_cols=[], seed=0, with_class=False, data_dim=2 ):\n",
    "\n",
    "    # Slightly slower code to create dataframes to feed into the xgboost dmatrix formats\n",
    "    \n",
    "    # real_samples = pd.DataFrame(x, columns=data_cols+label_cols)\n",
    "    # gen_samples = pd.DataFrame(g_z, columns=data_cols+label_cols)\n",
    "    # real_samples['syn_label'] = 0\n",
    "    # gen_samples['syn_label'] = 1\n",
    "    \n",
    "    # training_fraction = 0.5\n",
    "    # n_real, n_gen = int(len(real_samples)*training_fraction), int(len(gen_samples)*training_fraction)\n",
    "    # train_df = pd.concat([real_samples[:n_real],gen_samples[:n_gen]],axis=0)\n",
    "    # test_df = pd.concat([real_samples[n_real:],gen_samples[n_gen:]],axis=0)\n",
    "\n",
    "    # X_col = test_df.columns[:-1]\n",
    "    # y_col = test_df.columns[-1]\n",
    "    # dtrain = xgb.DMatrix(train_df[X_col], train_df[y_col], feature_names=X_col)\n",
    "    # dtest = xgb.DMatrix(test_df[X_col], feature_names=X_col)\n",
    "    # y_true = test_df['syn_label']\n",
    "    dtrain = np.vstack( [ x[:int(len(x)/2)], g_z[:int(len(g_z)/2)] ] ) # Use half of each real and generated set for training\n",
    "    dlabels = np.hstack( [ np.zeros(int(len(x)/2)), np.ones(int(len(g_z)/2)) ] ) # synthetic labels\n",
    "    dtest = np.vstack( [ x[int(len(x)/2):], g_z[int(len(g_z)/2):] ] ) # Use the other half of each set for testing\n",
    "    y_true = dlabels # Labels for test samples will be the same as the labels for training samples, assuming even batch sizes\n",
    "    \n",
    "    dtrain = xgb.DMatrix(dtrain, dlabels, feature_names=data_cols+label_cols)\n",
    "    dtest = xgb.DMatrix(dtest, feature_names=data_cols+label_cols)\n",
    "    \n",
    "    xgb_params = {\n",
    "        # 'tree_method': 'hist', # for faster evaluation\n",
    "        'max_depth': 4, # for faster evaluation\n",
    "        'objective': 'binary:logistic',\n",
    "        'random_state': 0,\n",
    "        'eval_metric': 'auc', # allows for balanced or unbalanced classes \n",
    "        }\n",
    "    xgb_test = xgb.train(xgb_params, dtrain, num_boost_round=10) # limit to ten rounds for faster evaluation\n",
    "    y_pred = np.round(xgb_test.predict(dtest))\n",
    "\n",
    "    # return '{:.2f}'.format(SimpleAccuracy(y_pred, y_true)) # assumes balanced real and generated datasets\n",
    "    return SimpleAccuracy(y_pred, y_true) # assumes balanced real and generated datasets\n",
    "    \n",
    "def PlotData( x, g_z, data_cols, label_cols=[], seed=0, with_class=False, data_dim=2, save=False, prefix='' ):\n",
    "    \n",
    "    real_samples = pd.DataFrame(x, columns=data_cols+label_cols)\n",
    "    gen_samples = pd.DataFrame(g_z, columns=data_cols+label_cols)\n",
    "    \n",
    "    f, axarr = plt.subplots(1, 2, figsize=(6,2) )\n",
    "    if with_class:\n",
    "        axarr[0].scatter( real_samples[data_cols[0]], real_samples[data_cols[1]], c=real_samples[label_cols[0]]/2 ) #, cmap='plasma'  )\n",
    "        axarr[1].scatter( gen_samples[ data_cols[0]], gen_samples[ data_cols[1]], c=gen_samples[label_cols[0]]/2 ) #, cmap='plasma'  )\n",
    "        \n",
    "        # For when there are multiple one-hot encoded label columns\n",
    "        # for i in range(len(label_cols)):\n",
    "            # temp = real_samples.loc[ real_samples[ label_cols[i] ] == 1 ]\n",
    "            # axarr[0].scatter( temp[data_cols[0]], temp[data_cols[1]], c='C'+str(i), label=i )\n",
    "            # temp = gen_samples.loc[ gen_samples[ label_cols[i] ] == 1 ]\n",
    "            # axarr[1].scatter( temp[data_cols[0]], temp[data_cols[1]], c='C'+str(i), label=i )\n",
    "        \n",
    "    else:\n",
    "        axarr[0].scatter( real_samples[data_cols[0]], real_samples[data_cols[1]]) #, cmap='plasma'  )\n",
    "        axarr[1].scatter( gen_samples[data_cols[0]], gen_samples[data_cols[1]]) #, cmap='plasma'  )\n",
    "    axarr[0].set_title('real')\n",
    "    axarr[1].set_title('generated')   \n",
    "    axarr[0].set_ylabel(data_cols[1]) # Only add y label to left plot\n",
    "    for a in axarr: a.set_xlabel(data_cols[0]) # Add x label to both plots\n",
    "    axarr[1].set_xlim(axarr[0].get_xlim()), axarr[1].set_ylim(axarr[0].get_ylim()) # Use axes ranges from real data for generated data\n",
    "    \n",
    "    if save:\n",
    "        plt.save( prefix + '.xgb_check.png' )\n",
    "        \n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to define the layers of the networks used in the 'define_models' functions below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def generator_network(x, data_dim, base_n_count): \n",
    "    x = layers.Dense(base_n_count, activation='relu')(x)\n",
    "    x = layers.Dense(base_n_count*2, activation='relu')(x)\n",
    "    x = layers.Dense(base_n_count*4, activation='relu')(x)\n",
    "    x = layers.Dense(data_dim)(x)    \n",
    "    return x\n",
    "    \n",
    "def generator_network_w_label(x, labels, data_dim, label_dim, base_n_count): \n",
    "    x = layers.concatenate([x,labels])\n",
    "    x = layers.Dense(base_n_count*1, activation='relu')(x) # 1\n",
    "    x = layers.Dense(base_n_count*2, activation='relu')(x) # 2\n",
    "    x = layers.Dense(base_n_count*4, activation='relu')(x)\n",
    "    # x = layers.Dense(base_n_count*4, activation='relu')(x) # extra\n",
    "    # x = layers.Dense(base_n_count*4, activation='relu')(x) # extra\n",
    "    x = layers.Dense(data_dim)(x)    \n",
    "    x = layers.concatenate([x,labels])\n",
    "    return x\n",
    "    \n",
    "def discriminator_network(x, data_dim, base_n_count):\n",
    "    x = layers.Dense(base_n_count*4, activation='relu')(x)\n",
    "    # x = layers.Dropout(0.1)(x)\n",
    "    x = layers.Dense(base_n_count*2, activation='relu')(x)\n",
    "    # x = layers.Dropout(0.1)(x)\n",
    "    x = layers.Dense(base_n_count, activation='relu')(x)\n",
    "    x = layers.Dense(1, activation='sigmoid')(x)\n",
    "    # x = layers.Dense(1)(x)\n",
    "    return x\n",
    "    \n",
    "def critic_network(x, data_dim, base_n_count):\n",
    "    x = layers.Dense(base_n_count*4, activation='relu')(x)\n",
    "    # x = layers.Dropout(0.1)(x)\n",
    "    x = layers.Dense(base_n_count*2, activation='relu')(x) # 2\n",
    "    # x = layers.Dropout(0.1)(x)\n",
    "    x = layers.Dense(base_n_count*1, activation='relu')(x) # 1\n",
    "    # x = layers.Dense(base_n_count*4, activation='relu')(x) # extra\n",
    "    # x = layers.Dense(base_n_count*4, activation='relu')(x) # extra\n",
    "    # x = layers.Dense(1, activation='sigmoid')(x)\n",
    "    x = layers.Dense(1)(x)\n",
    "    return x\n",
    "    \n",
    "    \n",
    "#### Functions to define the keras network models    \n",
    "    \n",
    "def define_models_GAN(rand_dim, data_dim, base_n_count, type=None):\n",
    "    generator_input_tensor = layers.Input(shape=(rand_dim, ))\n",
    "    generated_image_tensor = generator_network(generator_input_tensor, data_dim, base_n_count)\n",
    "    generated_or_real_image_tensor = layers.Input(shape=(data_dim,))\n",
    "    \n",
    "    if type == 'Wasserstein':\n",
    "        discriminator_output = critic_network(generated_or_real_image_tensor, data_dim, base_n_count)\n",
    "    else:\n",
    "        discriminator_output = discriminator_network(generated_or_real_image_tensor, data_dim, base_n_count)\n",
    "    generator_model = models.Model(inputs=[generator_input_tensor], outputs=[generated_image_tensor], name='generator')\n",
    "    discriminator_model = models.Model(inputs=[generated_or_real_image_tensor],\n",
    "                                       outputs=[discriminator_output],\n",
    "                                       name='discriminator')\n",
    "    combined_output = discriminator_model(generator_model(generator_input_tensor))\n",
    "    combined_model = models.Model(inputs=[generator_input_tensor], outputs=[combined_output], name='combined')\n",
    "    \n",
    "    return generator_model, discriminator_model, combined_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_models_CGAN(rand_dim, data_dim, label_dim, base_n_count, type=None):\n",
    "    generator_input_tensor = layers.Input(shape=(rand_dim, ))\n",
    "    labels_tensor = layers.Input(shape=(label_dim,)) # updated for class\n",
    "    generated_image_tensor = generator_network_w_label(generator_input_tensor, labels_tensor, data_dim, label_dim, base_n_count) # updated for class\n",
    "    generated_or_real_image_tensor = layers.Input(shape=(data_dim + label_dim,)) # updated for class\n",
    "    \n",
    "    if type == 'Wasserstein':\n",
    "        discriminator_output = critic_network(generated_or_real_image_tensor, data_dim + label_dim, base_n_count) # updated for class\n",
    "    else:\n",
    "        discriminator_output = discriminator_network(generated_or_real_image_tensor, data_dim + label_dim, base_n_count) # updated for class\n",
    "    generator_model = models.Model(inputs=[generator_input_tensor, labels_tensor], outputs=[generated_image_tensor], name='generator') # updated for class\n",
    "    discriminator_model = models.Model(inputs=[generated_or_real_image_tensor],\n",
    "                                       outputs=[discriminator_output],\n",
    "                                       name='discriminator')\n",
    "    combined_output = discriminator_model(generator_model([generator_input_tensor, labels_tensor])) # updated for class\n",
    "    combined_model = models.Model(inputs=[generator_input_tensor, labels_tensor], outputs=[combined_output], name='combined') # updated for class\n",
    "    \n",
    "    return generator_model, discriminator_model, combined_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions specific to the WGAN architecture<br>\n",
    "## The train discrimnator step is separated out to facilitate pre-training of the discriminator by itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def em_loss(y_coefficients, y_pred):\n",
    "    # define earth mover distance (wasserstein loss)\n",
    "    # literally the weighted average of the critic network output\n",
    "    # this is defined separately so it can be fed as a loss function to the optimizer in the WGANs\n",
    "    return tf.reduce_mean(tf.multiply(y_coefficients, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_discriminator_step(model_components, seed=0):\n",
    "    \n",
    "    [ cache_prefix, with_class, starting_step,\n",
    "                        train, data_cols, data_dim,\n",
    "                        label_cols, label_dim,\n",
    "                        generator_model, discriminator_model, combined_model,\n",
    "                        rand_dim, nb_steps, batch_size, \n",
    "                        k_d, k_g, critic_pre_train_steps, log_interval, learning_rate, base_n_count,\n",
    "                        data_dir, generator_model_path, discriminator_model_path,\n",
    "                        session, _z, _x, _labels, _g_z, epsilon, x_hat, gradients, _gradient_penalty,\n",
    "                        _disc_loss_generated, _disc_loss_real, _disc_loss, disc_optimizer,\n",
    "                        show,\n",
    "                        combined_loss, disc_loss_generated, disc_loss_real, xgb_losses\n",
    "                        ] = model_components\n",
    "    \n",
    "    if with_class:\n",
    "        d_l_g, d_l_r, _ = session.run([_disc_loss_generated, _disc_loss_real, disc_optimizer], feed_dict={\n",
    "            _z: np.random.normal(size=(batch_size, rand_dim)),\n",
    "            _x: get_data_batch(train, batch_size, seed=seed),\n",
    "            _labels: get_data_batch(train, batch_size, seed=seed)[:,-label_dim:], # .reshape(-1,label_dim), # updated for class            \n",
    "            epsilon: np.random.uniform(low=0.0, high=1.0, size=(batch_size, 1))\n",
    "        })\n",
    "    else:\n",
    "        d_l_g, d_l_r, _ = session.run([_disc_loss_generated, _disc_loss_real, disc_optimizer], feed_dict={\n",
    "            _z: np.random.normal(size=(batch_size, rand_dim)),\n",
    "            _x: get_data_batch(train, batch_size, seed=seed),\n",
    "            epsilon: np.random.uniform(low=0.0, high=1.0, size=(batch_size, 1))\n",
    "        })\n",
    "    return d_l_g, d_l_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_steps_WGAN(model_components):\n",
    "    \n",
    "    [ cache_prefix, with_class, starting_step,\n",
    "                        train, data_cols, data_dim,\n",
    "                        label_cols, label_dim,\n",
    "                        generator_model, discriminator_model, combined_model,\n",
    "                        rand_dim, nb_steps, batch_size, \n",
    "                        k_d, k_g, critic_pre_train_steps, log_interval, learning_rate, base_n_count,\n",
    "                        data_dir, generator_model_path, discriminator_model_path,\n",
    "                        session, _z, _x, _labels, _g_z, epsilon, x_hat, gradients, _gradient_penalty,\n",
    "                        _disc_loss_generated, _disc_loss_real, _disc_loss, disc_optimizer,\n",
    "                        show,\n",
    "                        combined_loss, disc_loss_generated, disc_loss_real, xgb_losses\n",
    "                        ] = model_components\n",
    "    \n",
    "    for i in range(starting_step, starting_step+nb_steps):\n",
    "        K.set_learning_phase(1) # 1 = train\n",
    "\n",
    "        # train the discriminator\n",
    "        for j in range(k_d):\n",
    "            d_l_g, d_l_r = train_discriminator_step(model_components, seed=i+j)\n",
    "        disc_loss_generated.append(d_l_g)\n",
    "        disc_loss_real.append(d_l_r)\n",
    "\n",
    "        # train the generator\n",
    "        for j in range(k_g):\n",
    "            np.random.seed(i+j)\n",
    "            z = np.random.normal(size=(batch_size, rand_dim))\n",
    "            if with_class:\n",
    "                labels = get_data_batch(train, batch_size, seed=i+j)[:,-label_dim:] # updated for class\n",
    "                loss = combined_model.train_on_batch([z, labels], [-np.ones(batch_size)]) # updated for class\n",
    "            else:\n",
    "                loss = combined_model.train_on_batch(z, [-np.ones(batch_size)])\n",
    "        combined_loss.append(loss)\n",
    "\n",
    "        # Determine xgb loss each step, after training generator and discriminator\n",
    "        if not i % 10: # 2x faster than testing each step...\n",
    "            K.set_learning_phase(0) # 0 = test\n",
    "            test_size = 492 # test using all of the actual fraud data\n",
    "            x = get_data_batch(train, test_size, seed=i)\n",
    "            z = np.random.normal(size=(test_size, rand_dim))\n",
    "            if with_class:\n",
    "                labels = x[:,-label_dim:]\n",
    "                g_z = generator_model.predict([z, labels])\n",
    "            else:\n",
    "                g_z = generator_model.predict(z)\n",
    "            xgb_loss = CheckAccuracy( x, g_z, data_cols, label_cols, seed=0, with_class=with_class, data_dim=data_dim )\n",
    "            xgb_losses = np.append(xgb_losses, xgb_loss)\n",
    "        \n",
    "        if not i % log_interval:\n",
    "            print('Step: {} of {}.'.format(i, starting_step + nb_steps))\n",
    "            # K.set_learning_phase(0) # 0 = test\n",
    "                        \n",
    "            # loss summaries   \n",
    "            print( 'Losses: G, D Gen, D Real, Xgb: {:.4f}, {:.4f}, {:.4f}, {:.4f}'.format(combined_loss[-1], disc_loss_generated[-1], disc_loss_real[-1], xgb_losses[-1]) )\n",
    "            print( 'D Real - D Gen: {:.4f}'.format(disc_loss_real[-1]-disc_loss_generated[-1]) )\n",
    "            # print('Generator model loss: {}.'.format(combined_loss[-1]))\n",
    "            # print('Discriminator model loss gen: {}.'.format(disc_loss_generated[-1]))\n",
    "            # print('Discriminator model loss real: {}.'.format(disc_loss_real[-1]))\n",
    "            # print('xgboost accuracy: {}'.format(xgb_losses[-1]) )\n",
    "            \n",
    "            if show:\n",
    "                PlotData( x, g_z, data_cols, label_cols, seed=0, with_class=with_class, data_dim=data_dim, \n",
    "                            save=False, prefix= data_dir + cache_prefix + '_' + str(i) )\n",
    "\n",
    "            # save model checkpoints\n",
    "            model_checkpoint_base_name = data_dir + cache_prefix + '_{}_model_weights_step_{}.h5'\n",
    "            generator_model.save_weights(model_checkpoint_base_name.format('generator', i))\n",
    "            discriminator_model.save_weights(model_checkpoint_base_name.format('discriminator', i))\n",
    "            pickle.dump([combined_loss, disc_loss_generated, disc_loss_real, xgb_losses], \n",
    "                open( data_dir + cache_prefix + '_losses_step_{}.pkl'.format(i) ,'wb'))\n",
    "    \n",
    "    return [combined_loss, disc_loss_generated, disc_loss_real, xgb_losses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversarial_training_WGAN(arguments, train, data_cols, label_cols=[], seed=0, starting_step=0):\n",
    "    [rand_dim, nb_steps, batch_size, \n",
    "             k_d, k_g, critic_pre_train_steps, log_interval, learning_rate, base_n_count,\n",
    "            data_dir, generator_model_path, discriminator_model_path, loss_pickle_path, show ] = arguments\n",
    "            \n",
    "    np.random.seed(seed)     # set random seed\n",
    "    \n",
    "    data_dim = len(data_cols)\n",
    "    print('data_dim: ', data_dim)\n",
    "    print('data_cols: ', data_cols)\n",
    "    \n",
    "    label_dim = 0\n",
    "    with_class = False\n",
    "    if len(label_cols) > 0: \n",
    "        with_class = True\n",
    "        label_dim = len(label_cols)\n",
    "        print('label_dim: ', label_dim)\n",
    "        print('label_cols: ', label_cols)\n",
    "    \n",
    "    # define network models\n",
    "    \n",
    "    K.set_learning_phase(1) # 1 = train\n",
    "    \n",
    "    if with_class:\n",
    "        cache_prefix = 'WCGAN'\n",
    "        generator_model, discriminator_model, combined_model = define_models_CGAN(rand_dim, data_dim, label_dim, base_n_count, type='Wasserstein')\n",
    "    else:\n",
    "        cache_prefix = 'WGAN'\n",
    "        generator_model, discriminator_model, combined_model = define_models_GAN(rand_dim, data_dim, base_n_count, type='Wasserstein')\n",
    "    \n",
    "    \n",
    "    # construct computation graph for calculating the gradient penalty (improved WGAN) and training the discriminator\n",
    "    _z = tf.placeholder(tf.float32, shape=(batch_size, rand_dim))\n",
    "    _labels = None\n",
    "    if with_class:  \n",
    "        _x = tf.placeholder(tf.float32, shape=(batch_size, data_dim + label_dim))    \n",
    "        _labels = tf.placeholder(tf.float32, shape=(batch_size, label_dim)) # updated for class\n",
    "        _g_z = generator_model(inputs=[_z, _labels]) # updated for class    \n",
    "    else:      \n",
    "        _x = tf.placeholder(tf.float32, shape=(batch_size, data_dim))\n",
    "        _g_z = generator_model(_z)\n",
    "    \n",
    "    epsilon = tf.placeholder(tf.float32, shape=(batch_size, 1))\n",
    "    \n",
    "    x_hat = epsilon * _x + (1.0 - epsilon) * _g_z\n",
    "    gradients = tf.gradients(discriminator_model(x_hat), [x_hat])\n",
    "    _gradient_penalty = 10.0 * tf.square(tf.norm(gradients[0], ord=2) - 1.0)\n",
    "    # calculate discriminator's loss\n",
    "    _disc_loss_generated = em_loss(tf.ones(batch_size), discriminator_model(_g_z))\n",
    "    _disc_loss_real = em_loss(tf.ones(batch_size), discriminator_model(_x))\n",
    "    _disc_loss = _disc_loss_generated - _disc_loss_real + _gradient_penalty\n",
    "\n",
    "    # update f by taking an SGD step on mini-batch loss LD(f)\n",
    "    disc_optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.5, beta2=0.9).minimize(\n",
    "        _disc_loss, var_list=discriminator_model.trainable_weights)\n",
    "    sess = K.get_session()\n",
    "\n",
    "    # compile models\n",
    "    session = K.get_session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    session.run(init)\n",
    "    adam = optimizers.Adam(lr=learning_rate, beta_1=0.5, beta_2=0.9)\n",
    "    discriminator_model.trainable = False\n",
    "    combined_model.compile(optimizer=adam, loss=[em_loss])\n",
    "    combined_loss, disc_loss_generated, disc_loss_real, xgb_losses = [], [], [], []\n",
    "    \n",
    "    model_components = [ cache_prefix, with_class, starting_step,\n",
    "                        train, data_cols, data_dim,\n",
    "                        label_cols, label_dim,\n",
    "                        generator_model, discriminator_model, combined_model,\n",
    "                        rand_dim, nb_steps, batch_size, \n",
    "                        k_d, k_g, critic_pre_train_steps, log_interval, learning_rate, base_n_count,\n",
    "                        data_dir, generator_model_path, discriminator_model_path,\n",
    "                        session, _z, _x, _labels, _g_z, epsilon, x_hat, gradients, _gradient_penalty,\n",
    "                        _disc_loss_generated, _disc_loss_real, _disc_loss, disc_optimizer,\n",
    "                        show,\n",
    "                        combined_loss, disc_loss_generated, disc_loss_real, xgb_losses\n",
    "                        ]\n",
    "    if show:\n",
    "        print(generator_model.summary())\n",
    "        print(discriminator_model.summary())\n",
    "        print(combined_model.summary())\n",
    "    if loss_pickle_path:\n",
    "        print('Loading loss pickles')\n",
    "        [combined_loss, disc_loss_generated, disc_loss_real, xgb_losses] = pickle.load(open(loss_pickle_path,'rb'))\n",
    "    if generator_model_path:\n",
    "        print('Loading generator model')\n",
    "        generator_model.load_weights(generator_model_path) #, by_name=True)\n",
    "    if discriminator_model_path:\n",
    "        print('Loading discriminator model')\n",
    "        discriminator_model.load_weights(discriminator_model_path) #, by_name=True)\n",
    "    else:\n",
    "        print('pre-training the critic...')\n",
    "        K.set_learning_phase(1) # 1 = train\n",
    "        for i in range(critic_pre_train_steps):\n",
    "            if i%20==0:\n",
    "                print('Step: {} of {} critic pre-training.'.format(i, critic_pre_train_steps))\n",
    "            loss = train_discriminator_step(model_components, seed=i)\n",
    "        print('Last batch of critic pre-training disc_loss: {}.'.format(loss))\n",
    "    model_components = [ cache_prefix, with_class, starting_step,\n",
    "                        train, data_cols, data_dim,\n",
    "                        label_cols, label_dim,\n",
    "                        generator_model, discriminator_model, combined_model,\n",
    "                        rand_dim, nb_steps, batch_size, \n",
    "                        k_d, k_g, critic_pre_train_steps, log_interval, learning_rate, base_n_count,\n",
    "                        data_dir, generator_model_path, discriminator_model_path,\n",
    "                        session, _z, _x, _labels, _g_z, epsilon, x_hat, gradients, _gradient_penalty,\n",
    "                        _disc_loss_generated, _disc_loss_real, _disc_loss, disc_optimizer,\n",
    "                        show,\n",
    "                        combined_loss, disc_loss_generated, disc_loss_real, xgb_losses\n",
    "                        ]\n",
    "        \n",
    "    [combined_loss, disc_loss_generated, disc_loss_real, xgb_losses] = training_steps_WGAN(model_components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions specific to the vanilla GAN architecture   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "def training_steps_GAN(model_components):\n",
    "    \n",
    "    [ cache_prefix, with_class, starting_step,\n",
    "                        train, data_cols, data_dim,\n",
    "                        label_cols, label_dim,\n",
    "                        generator_model, discriminator_model, combined_model,\n",
    "                        rand_dim, nb_steps, batch_size, \n",
    "                        k_d, k_g, critic_pre_train_steps, log_interval, learning_rate, base_n_count,\n",
    "                        data_dir, generator_model_path, discriminator_model_path, show,\n",
    "                        combined_loss, disc_loss_generated, disc_loss_real, xgb_losses ] = model_components  \n",
    "    \n",
    "    for i in range(starting_step, starting_step+nb_steps):\n",
    "        K.set_learning_phase(1) # 1 = train\n",
    "\n",
    "        # train the discriminator\n",
    "        for j in range(k_d):\n",
    "            np.random.seed(i+j)\n",
    "            z = np.random.normal(size=(batch_size, rand_dim))\n",
    "            x = get_data_batch(train, batch_size, seed=i+j)\n",
    "            \n",
    "            if with_class:\n",
    "                labels = x[:,-label_dim:]\n",
    "                g_z = generator_model.predict([z, labels])\n",
    "            else:\n",
    "                g_z = generator_model.predict(z)\n",
    "#             x = np.vstack([x,g_z]) # code to train the discriminator on real and generated data at the same time, but you have to run network again for separate losses\n",
    "#             classes = np.hstack([np.zeros(batch_size),np.ones(batch_size)])\n",
    "#             d_l_r = discriminator_model.train_on_batch(x, classes)\n",
    "            \n",
    "            d_l_r = discriminator_model.train_on_batch(x, np.random.uniform(low=0.999, high=1.0, size=batch_size)) # 0.7, 1.2 # GANs need noise to prevent loss going to zero\n",
    "            d_l_g = discriminator_model.train_on_batch(g_z, np.random.uniform(low=0.0, high=0.001, size=batch_size)) # 0.0, 0.3 # GANs need noise to prevent loss going to zero\n",
    "            # d_l_r = discriminator_model.train_on_batch(x, np.ones(batch_size)) # without noise\n",
    "            # d_l_g = discriminator_model.train_on_batch(g_z, np.zeros(batch_size)) # without noise\n",
    "        disc_loss_real.append(d_l_r)\n",
    "        disc_loss_generated.append(d_l_g)\n",
    "        \n",
    "        # train the generator\n",
    "        for j in range(k_g):\n",
    "            np.random.seed(i+j)\n",
    "            z = np.random.normal(size=(batch_size, rand_dim))\n",
    "            if with_class:\n",
    "                # loss = combined_model.train_on_batch([z, labels], np.ones(batch_size)) # without noise\n",
    "                loss = combined_model.train_on_batch([z, labels], np.random.uniform(low=0.999, high=1.0, size=batch_size)) # 0.7, 1.2 # GANs need noise to prevent loss going to zero\n",
    "            else:\n",
    "                # loss = combined_model.train_on_batch(z, np.ones(batch_size)) # without noise\n",
    "                loss = combined_model.train_on_batch(z, np.random.uniform(low=0.999, high=1.0, size=batch_size)) # 0.7, 1.2 # GANs need noise to prevent loss going to zero\n",
    "        combined_loss.append(loss)\n",
    "        \n",
    "        # Determine xgb loss each step, after training generator and discriminator\n",
    "        if not i % 10: # 2x faster than testing each step...\n",
    "            K.set_learning_phase(0) # 0 = test\n",
    "            test_size = 492 # test using all of the actual fraud data\n",
    "            x = get_data_batch(train, test_size, seed=i)\n",
    "            z = np.random.normal(size=(test_size, rand_dim))\n",
    "            if with_class:\n",
    "                labels = x[:,-label_dim:]\n",
    "                g_z = generator_model.predict([z, labels])\n",
    "            else:\n",
    "                g_z = generator_model.predict(z)\n",
    "            xgb_loss = CheckAccuracy( x, g_z, data_cols, label_cols, seed=0, with_class=with_class, data_dim=data_dim )\n",
    "            xgb_losses = np.append(xgb_losses, xgb_loss)\n",
    "\n",
    "        # Saving weights and plotting images\n",
    "        if not i % log_interval:\n",
    "            print('Step: {} of {}.'.format(i, starting_step + nb_steps))\n",
    "            K.set_learning_phase(0) # 0 = test\n",
    "                        \n",
    "            # loss summaries      \n",
    "            print( 'Losses: G, D Gen, D Real, Xgb: {:.4f}, {:.4f}, {:.4f}, {:.4f}'.format(combined_loss[-1], disc_loss_generated[-1], disc_loss_real[-1], xgb_losses[-1]) )\n",
    "            print( 'D Real - D Gen: {:.4f}'.format(disc_loss_real[-1]-disc_loss_generated[-1]) )            \n",
    "            # print('Generator model loss: {}.'.format(combined_loss[-1]))\n",
    "            # print('Discriminator model loss gen: {}.'.format(disc_loss_generated[-1]))\n",
    "            # print('Discriminator model loss real: {}.'.format(disc_loss_real[-1]))\n",
    "            # print('xgboost accuracy: {}'.format(xgb_losses[-1]) )\n",
    "            \n",
    "            if show:\n",
    "                PlotData( x, g_z, data_cols, label_cols, seed=0, with_class=with_class, data_dim=data_dim, \n",
    "                            save=False, prefix= data_dir + cache_prefix + '_' + str(i) )\n",
    "            \n",
    "            # save model checkpoints\n",
    "            model_checkpoint_base_name = data_dir + cache_prefix + '_{}_model_weights_step_{}.h5'\n",
    "            generator_model.save_weights(model_checkpoint_base_name.format('generator', i))\n",
    "            discriminator_model.save_weights(model_checkpoint_base_name.format('discriminator', i))\n",
    "            pickle.dump([combined_loss, disc_loss_generated, disc_loss_real, xgb_losses], \n",
    "                open( data_dir + cache_prefix + '_losses_step_{}.pkl'.format(i) ,'wb'))\n",
    "    \n",
    "    return [combined_loss, disc_loss_generated, disc_loss_real, xgb_losses]\n",
    "    \n",
    "def adversarial_training_GAN(arguments, train, data_cols, label_cols=[], seed=0, starting_step=0):\n",
    "    [rand_dim, nb_steps, batch_size, \n",
    "             k_d, k_g, critic_pre_train_steps, log_interval, learning_rate, base_n_count,\n",
    "            data_dir, generator_model_path, discriminator_model_path, loss_pickle_path, show ] = arguments\n",
    "    \n",
    "    np.random.seed(seed)     # set random seed\n",
    "    \n",
    "    data_dim = len(data_cols)\n",
    "    print('data_dim: ', data_dim)\n",
    "    print('data_cols: ', data_cols)\n",
    "    \n",
    "    label_dim = 0\n",
    "    with_class = False\n",
    "    if len(label_cols) > 0: \n",
    "        with_class = True\n",
    "        label_dim = len(label_cols)\n",
    "        print('label_dim: ', label_dim)\n",
    "        print('label_cols: ', label_cols)\n",
    "    \n",
    "    # define network models\n",
    "    \n",
    "    K.set_learning_phase(1) # 1 = train\n",
    "    \n",
    "    if with_class:\n",
    "        cache_prefix = 'CGAN'\n",
    "        generator_model, discriminator_model, combined_model = define_models_CGAN(rand_dim, data_dim, label_dim, base_n_count)\n",
    "    else:\n",
    "        cache_prefix = 'GAN'\n",
    "        generator_model, discriminator_model, combined_model = define_models_GAN(rand_dim, data_dim, base_n_count)\n",
    "    \n",
    "    # compile models\n",
    "    adam = optimizers.Adam(lr=learning_rate, beta_1=0.5, beta_2=0.9)\n",
    "    generator_model.compile(optimizer=adam, loss='binary_crossentropy')\n",
    "    discriminator_model.compile(optimizer=adam, loss='binary_crossentropy')\n",
    "    discriminator_model.trainable = False\n",
    "    combined_model.compile(optimizer=adam, loss='binary_crossentropy')\n",
    "    \n",
    "    if show:\n",
    "        print(generator_model.summary())\n",
    "        print(discriminator_model.summary())\n",
    "        print(combined_model.summary())\n",
    "    combined_loss, disc_loss_generated, disc_loss_real, xgb_losses = [], [], [], []\n",
    "    \n",
    "    if loss_pickle_path:\n",
    "        print('Loading loss pickles')\n",
    "        [combined_loss, disc_loss_generated, disc_loss_real, xgb_losses] = pickle.load(open(loss_pickle_path,'rb'))\n",
    "    if generator_model_path:\n",
    "        print('Loading generator model')\n",
    "        generator_model.load_weights(generator_model_path, by_name=True)\n",
    "    if discriminator_model_path:\n",
    "        print('Loading discriminator model')\n",
    "        discriminator_model.load_weights(discriminator_model_path, by_name=True)\n",
    "    model_components = [ cache_prefix, with_class, starting_step,\n",
    "                        train, data_cols, data_dim,\n",
    "                        label_cols, label_dim,\n",
    "                        generator_model, discriminator_model, combined_model,\n",
    "                        rand_dim, nb_steps, batch_size, \n",
    "                        k_d, k_g, critic_pre_train_steps, log_interval, learning_rate, base_n_count,\n",
    "                        data_dir, generator_model_path, discriminator_model_path, show,\n",
    "                        combined_loss, disc_loss_generated, disc_loss_real, xgb_losses ]\n",
    "        \n",
    "    [combined_loss, disc_loss_generated, disc_loss_real, xgb_losses] = training_steps_GAN(model_components)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "def sample_z(m, n): # updated to normal distribution\n",
    "#     return np.random.uniform(-1., 1., size=[m, n])\n",
    "    return np.random.normal(size=[m, n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_init(size): # updated to uniform distribution using standard xavier formulation\n",
    "#     in_dim = size[0]\n",
    "#     xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
    "#     return tf.random_normal(shape=size, stddev=xavier_stddev, seed=global_seed)\n",
    "    xavier_range = tf.sqrt( 6 / ( size[0] + size[1] ) )\n",
    "    return tf.random_uniform(shape=size, minval=-xavier_range, maxval=xavier_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# End of function list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
